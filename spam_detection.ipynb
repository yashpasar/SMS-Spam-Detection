{"cells":[{"cell_type":"markdown","source":["# IST 718: Big Data Analytics\n\n- Professor: Willard Williamson <wewillia@syr.edu>\n- Faculty Assistant: Palaniappan Muthukkaruppan\n## General instructions:\n\n- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Any code is allowed from the class text books or class provided code.__\n- Please do not change the file names. The FAs and the professor use these names to grade your homework.\n- Remove or comment out code that contains `raise NotImplementedError`. This is mainly to make the `assert` statement fail if nothing is submitted.\n- The tests shown in some cells (i.e., `assert` and `np.testing.` statements) are used to grade your answers. **However, the professor and FAs will use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n- Before submitting your work through Blackboard, remember to save and press `Validate` (or go to \n`Kernel`$\\rightarrow$`Restart and Run All`)."],"metadata":{"deletable":false,"editable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-b038e38b5e3072a9","locked":true,"solution":false,"checksum":"c51f80b694da894627ba37be28c86055","grade":false}}},{"cell_type":"code","source":["# Load the packages needed for this part\n# create spark and sparkcontext objects\nfrom pyspark.sql import SparkSession\nimport numpy as np\n\nspark = SparkSession.builder.getOrCreate()\nsc = spark.sparkContext\n\nimport pyspark\nfrom pyspark.ml import feature, regression, Pipeline, classification, pipeline, evaluation\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.sql import functions as fn, Row\nfrom pyspark.sql.functions import when, regexp_extract, col\nfrom pyspark import sql\n\nimport matplotlib.pyplot as plt\nimport pandas as pd"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["In this section, you are going to develop a SMS spam detector based on logistic regression. This is the same idea behind sentiment analysis, but instead of predicting positive sentiment vs negative sentiment, you are going to predict whether a SMS text is spam or not.\n\nThe dataset will be in `sms_spam_df`"],"metadata":{}},{"cell_type":"code","source":["sms_spam_df = spark.read.csv('/FileStore/tables/sms_spam.csv', header=True, inferSchema=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["# Question 1.1 (10 pts)\n\nEncode the `type` column to be 1 for `spam` and 0 for `ham` and store the result in `sms_spam2_df`"],"metadata":{}},{"cell_type":"code","source":["# create sms_spam2_df below\nsms_spam2_df = sms_spam_df.withColumn(\"type\", fn.when(fn.col('type') == ('spam'),1).otherwise(0))\nsms_spam2_df.show()\n#raise NotImplementedError()"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-79e2472845514523","locked":false,"solution":true,"checksum":"ede01a8c8ef80d25d4fa8d9132b6f9c0","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------------------+\ntype|                text|\n+----+--------------------+\n   0|Go until jurong p...|\n   0|Ok lar... Joking ...|\n   1|Free entry in 2 a...|\n   0|U dun say so earl...|\n   0|Nah I don&apos;t think...|\n   1|FreeMsg Hey there...|\n   0|Even my brother i...|\n   0|As per your reque...|\n   1|WINNER!! As a val...|\n   1|Had your mobile 1...|\n   0|I&apos;m gonna be home...|\n   1|SIX chances to wi...|\n   1|URGENT! You have ...|\n   0|I&apos;ve been searchi...|\n   0|I HAVE A DATE ON ...|\n   1|XXXMobileMovieClu...|\n   0|Oh k...i&apos;m watchi...|\n   0|Eh u remember how...|\n   0|Fine if that&apos;s th...|\n   1|England v Macedon...|\n+----+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# (10 pts)\nnp.testing.assert_array_equal(\n    sms_spam2_df.groupBy('type').count().orderBy('type').rdd.map(lambda x: x['count']).collect(),\n    [4827, 747]\n)"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-c6048eb3c38d3030","locked":true,"solution":false,"points":5,"checksum":"3c39d9b0191d5b096f133b45d5f38f67","grade":true}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["# Question 1.2: tfidf feature engineering (15 pts)\nCreate a pipeline that combines a `Tokenizer`, `CounterVectorizer`, and a `IDF` estimator to compute the tfidf vectors of each SMS. Fit this pipeline and assign the pipeline transformer to a variable `tfidf_pipeline`. The `Tokenizer` step should create a column `words`, the `CounterVectorizer` step should create a column `tf`, and the `IDF` step should create a column `tfidf`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import Tokenizer\nfrom pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.feature import IDF"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# create a Pipeline transformer and name it tfidf_pipeline\ntokenizer = Tokenizer().setInputCol('text').setOutputCol('words')\nCounterVectorizer = CountVectorizer().setInputCol('words').setOutputCol('tf')\nIDF = IDF().setInputCol('tf').setOutputCol('tfidf')\ntfidf_pipeline = Pipeline(stages = [tokenizer, CounterVectorizer, IDF]).fit(sms_spam2_df)\n#raise NotImplementedError()"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-ab24110e63d19470","locked":false,"solution":true,"checksum":"521084e2063144fa97a14bef1e965fca","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["tfidf_pipeline.transform(sms_spam2_df).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------------------+--------------------+--------------------+--------------------+\ntype|                text|               words|                  tf|               tfidf|\n+----+--------------------+--------------------+--------------------+--------------------+\n   0|Go until jurong p...|[go, until, juron...|(13525,[8,42,51,6...|(13525,[8,42,51,6...|\n   0|Ok lar... Joking ...|[ok, lar..., joki...|(13525,[5,74,404,...|(13525,[5,74,404,...|\n   1|Free entry in 2 a...|[free, entry, in,...|(13525,[0,3,8,20,...|(13525,[0,3,8,20,...|\n   0|U dun say so earl...|[u, dun, say, so,...|(13525,[5,22,60,1...|(13525,[5,22,60,1...|\n   0|Nah I don&apos;t think...|[nah, i, don&apos;t, t...|(13525,[0,1,66,86...|(13525,[0,1,66,86...|\n+----+--------------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["# (15 pts)\nnp.testing.assert_array_equal([type(s) for s in tfidf_pipeline.stages],\n                              [feature.Tokenizer, feature.CountVectorizerModel, feature.IDFModel])"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-cd280a1ccf0f1705","locked":true,"solution":false,"points":5,"checksum":"34ef778045b84441f525a78f9db3eb70","grade":true}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Investigate the fitted pieline above and create a variable `lowest_idf` that contains the set of words with the 5 lowest IDF. **Hint: you must extract the vocabulary from the fitted `CountVectorizer` and the IDF values from the fitted `IDF`, both in the stages of `tfidf_pipeline`. You can put both lists into Pandas dataframe columns and sort by idf, picking 5 after sorting**"],"metadata":{}},{"cell_type":"code","source":["tfidf_pd_df = tfidf_pipeline.transform(sms_spam2_df).toPandas()\ntfidf_pd_df.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div style=\"max-width:1500px;overflow:auto;\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>text</th>\n      <th>words</th>\n      <th>tf</th>\n      <th>tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>[go, until, jurong, point,, crazy.., available...</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.975...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>[ok, lar..., joking, wif, u, oni...]</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 2.02617709712, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n      <td>(3.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n      <td>(3.60564694586, 0.0, 0.0, 1.56987231223, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>[u, dun, say, so, early, hor..., u, c, already...</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, ...</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 4.05235419423, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>[nah, i, don't, think, he, goes, to, usf,, he,...</td>\n      <td>(1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>(1.20188231529, 1.25016544811, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["# create lower_idf with list of words with 5 lowest IDF values.\nvocab = tfidf_pipeline.stages[1].vocabulary\nidf = tfidf_pipeline.stages[-1].idf.toArray()\nlowest_idf = pd.DataFrame({'Vocab': vocab, 'IDF': idf})\nlowest_idf = lowest_idf.sort_values('IDF')\nlowest_idf = set(lowest_idf.Vocab.head())\n#raise NotImplementedError()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["# (5 pts)\n# it is a set\nnp.testing.assert_equal(type(lowest_idf), set)\n# it has 5 elements\nnp.testing.assert_equal(len(lowest_idf), 5)\n# each element is a string\nnp.testing.assert_equal({type(w) for w in lowest_idf}, {str})"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-5493b6e9b55bd45f","locked":true,"solution":false,"points":5,"checksum":"faade02098f10061ba0107815cc4d81c","grade":true}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["# Question 1.3: uppercase feature (15 pts)\n\nTypical spam messages contain words that are upper case. Create a dataframe `sms_spam3_df` where you add a new column `has_uppercase` which contains an integer `1` if the first sequence of uppercase letters is longer or equal to 3 and an integer `0` otherwise."],"metadata":{}},{"cell_type":"code","source":["pd.options.display.max_colwidth = 500\npd.set_option('display.max_rows', None)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["# create sms_spam3_df below\nsms_spam3_df = sms_spam2_df.select(fn.col('type'), fn.col('text'), fn.when((fn.length(fn.regexp_extract(fn.col('text'),'[A-Z]{3,}',0))) >= 3,1).otherwise(0).alias('has_uppercase'))\n#raise NotImplementedError()"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-02520df239057529","locked":false,"solution":true,"checksum":"4a654d9e0b5d08d00aaaaa4cb3e04e30","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["sms_spam3_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------------------+-------------+\ntype|                text|has_uppercase|\n+----+--------------------+-------------+\n   0|Go until jurong p...|            0|\n   0|Ok lar... Joking ...|            0|\n   1|Free entry in 2 a...|            0|\n   0|U dun say so earl...|            0|\n   0|Nah I don&apos;t think...|            0|\n   1|FreeMsg Hey there...|            0|\n   0|Even my brother i...|            0|\n   0|As per your reque...|            0|\n   1|WINNER!! As a val...|            1|\n   1|Had your mobile 1...|            1|\n   0|I&apos;m gonna be home...|            0|\n   1|SIX chances to wi...|            1|\n   1|URGENT! You have ...|            1|\n   0|I&apos;ve been searchi...|            0|\n   0|I HAVE A DATE ON ...|            1|\n   1|XXXMobileMovieClu...|            1|\n   0|Oh k...i&apos;m watchi...|            0|\n   0|Eh u remember how...|            0|\n   0|Fine if that&apos;s th...|            0|\n   1|England v Macedon...|            1|\n+----+--------------------+-------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["The first three messages with `has_uppercase == 1` are as follows:\n\n```python\nsms_spam3_df.where('has_uppercase == 1').take(3)\n```\n\n```console\n[Row(type=1, text='WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.', has_uppercase=1),\n Row(type=1, text='Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030', has_uppercase=1),\n Row(type=1, text='SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info', has_uppercase=1)]\n```"],"metadata":{}},{"cell_type":"code","source":["# try it here\nsms_spam3_df.where('has_uppercase == 1').take(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">38</span><span class=\"ansired\">]: </span>[Row(has_uppercase=1, text=&apos;WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.&apos;, type=1),\n Row(has_uppercase=1, text=&apos;Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030&apos;, type=1),\n Row(has_uppercase=1, text=&apos;SIX chances to win CASH! From 100 to 20,000 pounds txt&gt; CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info&apos;, type=1)]</div>"]}}],"execution_count":22},{"cell_type":"code","source":["# (15 pts)\nnp.testing.assert_equal(set(sms_spam3_df.columns), {'has_uppercase', 'text', 'type'})\nnp.testing.assert_equal(type(sms_spam3_df.schema['has_uppercase'].dataType), sql.types.IntegerType)\nnp.testing.assert_equal(sms_spam3_df.rdd.map(lambda x : x['has_uppercase']).sum(), 891)"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-905724115443b1ad","locked":true,"solution":false,"points":5,"checksum":"e380d6f60200785939c9acae7e614bef","grade":true}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["# Question 1.4: Compare models (15 pts)\n\nUsing the following splits:"],"metadata":{}},{"cell_type":"code","source":["training_df, validation_df, testing_df = sms_spam2_df.randomSplit([0.6, 0.3, 0.1], seed=0)"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-2f05175cd6ae7f5c","locked":true,"solution":false,"checksum":"8293adada027540531a2727eff204198","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["[training_df.count(), validation_df.count(), testing_df.count()]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">41</span><span class=\"ansired\">]: </span>[3349, 1674, 551]</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["Create pipelines where the first stage is the `tfidf_pipeline` created above and the second stage is a `LogisticRegression` model with different regularization parameters ($\\lambda$) and elastic net mixture ($\\alpha$). Fit those pipelines to the appropriate data split.\n\n1. Logistic regression with $\\lambda=0$ and $\\alpha=0$ (assign the fitted pipeline to `lr_pipeline1`)\n2. Logistic regression with $\\lambda=0.02$ and $\\alpha=0.2$ (assign the fitted pipeline to `lr_pipeline2`)\n3. Logistic regression with $\\lambda=0.1$ and $\\alpha=0.4$ (assign the fitted pipeline to `lr_pipeline3`)"],"metadata":{}},{"cell_type":"code","source":["# create lr_pipeline1, lr_pipeline2, and lr_pipeline3\nlr1 = LogisticRegression().setLabelCol('type').setFeaturesCol('tfidf').setRegParam(0.0).setMaxIter(100).setElasticNetParam(0.0)\nlr2 = LogisticRegression().setLabelCol('type').setFeaturesCol('tfidf').setRegParam(0.02).setMaxIter(100).setElasticNetParam(0.2)\nlr3 = LogisticRegression().setLabelCol('type').setFeaturesCol('tfidf').setRegParam(0.1).setMaxIter(100).setElasticNetParam(0.4)\nlr_pipeline1 = Pipeline(stages=[tfidf_pipeline, lr1]).fit(training_df)\nlr_pipeline2 = Pipeline(stages=[tfidf_pipeline, lr2]).fit(training_df)\nlr_pipeline3 = Pipeline(stages=[tfidf_pipeline, lr3]).fit(training_df)\n#raise NotImplementedError()"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-8db1a50673eea1a7","locked":false,"solution":true,"checksum":"02b7fefd0dd482900ba644d56b02400a","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["# (15 pts)\nnp.testing.assert_equal(type(lr_pipeline1), pipeline.PipelineModel)\nnp.testing.assert_equal(type(lr_pipeline2), pipeline.PipelineModel)\nnp.testing.assert_equal(type(lr_pipeline3), pipeline.PipelineModel)\nnp.testing.assert_array_equal([type(s) for s in lr_pipeline1.stages],\n                              [pipeline.PipelineModel, classification.LogisticRegressionModel])\nnp.testing.assert_array_equal([type(s) for s in lr_pipeline2.stages],\n                              [pipeline.PipelineModel, classification.LogisticRegressionModel])\nnp.testing.assert_array_equal([type(s) for s in lr_pipeline3.stages],\n                              [pipeline.PipelineModel, classification.LogisticRegressionModel])"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-bc59c22c523a9016","locked":true,"solution":false,"points":5,"checksum":"5aaad4eb8cc1ba3b64fd719c3a154ca5","grade":true}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["Use the evaluator object defined below to compute the ROC AUC of your predictors. For example, to compute the AUC of pipeline 1 for a dataframe `df`, you would run\n\n```python\nevaluator.evaluate(lr_pipeline1.transform(df))\n```\n\nAssign the AUC of the three models to the variables `AUC1`, `AUC2`, and `AUC3`, and and assign the pipeline with the best model to a variable `best_model`"],"metadata":{}},{"cell_type":"code","source":["evaluator = evaluation.BinaryClassificationEvaluator(labelCol='type')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-44d4a941d4aef83e","locked":true,"solution":false,"checksum":"74638d21bd238bcc28672143908e8b8c","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["For example, the AUC on training of the first model is perfect:\n\n```\nevaluator.evaluate(lr_pipeline1.transform(training_df))\n```\n\n```console\n1.0\n```"],"metadata":{}},{"cell_type":"code","source":["# AUC code here\nAUC1 = evaluator.evaluate(lr_pipeline1.transform(validation_df))\nAUC2 = evaluator.evaluate(lr_pipeline2.transform(validation_df))\nAUC3 = evaluator.evaluate(lr_pipeline3.transform(validation_df))\n#raise NotImplementedError()"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-a9d883059572796b","locked":false,"solution":true,"checksum":"6d2b1fd1d3f5e9dbcd1986f7c6486293","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"code","source":["# (15 pts)\nnp.testing.assert_array_equal([type(AUC1), type(AUC2), type(AUC3)],\n                             [float, float, float])\n# AUC less than 1\nnp.testing.assert_array_less([AUC1, AUC2, AUC3], [1, 1, 1])\n# AUC more than 0.5\nnp.testing.assert_array_less([.5, .5, .5],\n                            [AUC1, AUC2, AUC3])"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-29d52d2cec5c8a1e","locked":true,"solution":false,"points":5,"checksum":"1c76a13060b46900aa055920ae5f6a2f","grade":true}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"code","source":["print(AUC1, AUC2, AUC3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.9557218489415972 0.9871555611031791 0.9686667539402503\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["# Create best_model variable here\nbest_model = lr_pipeline2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["# Question 1.5: Choose best model (15 pts)\n\nUsing the right split and the best model selected before, compute the generalization performance and assign it to a variable `AUC_best`"],"metadata":{}},{"cell_type":"code","source":["# Compute the AUC of the best model and assign to the variable AUC_best\nAUC_best = evaluator.evaluate(best_model.transform(testing_df))\n#raise NotImplementedError()"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-2d30a81f9c08fc5d","locked":false,"solution":true,"checksum":"7780410d49dd0727bea11bcfa2ec75de","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":38},{"cell_type":"code","source":["# (15 pts)\nnp.testing.assert_approx_equal(AUC_best, \n                               0.976126746201693, significant=2)"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-dc8fd7d7ce658642","locked":true,"solution":false,"points":5,"checksum":"77cef30af04406aeee070bae31e75129","grade":true}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["# Question 1.6: Inference (15 pts)\n\nUse the pipeline 2 fitted above (`lr_pipeline2`) to create Pandas dataframes that contain the most negative words and the most positive words. In particular, create a dataframe `positive_words` with the columns `word` and `weight` with the top 20 positive words, sorted by descending coefficient. Similarly create a `negative_words` Pandas dataframe with the top 20 negative words where the coefficient are sorted in ascending order. **Hint: Use the `sentiment_analysis.ipynb` notebook in the repo for inspiration**"],"metadata":{}},{"cell_type":"code","source":["# create positive_words and negative_words pandas dataframe below    \nvocabulary = tfidf_pipeline.stages[1].vocabulary\nweights = lr_pipeline2.stages[-1].coefficients.toArray()\ncoeffs_df = pd.DataFrame({'word': vocabulary, 'weight': weights})\npositive_words = coeffs_df.sort_values('weight', ascending=False).head(20)\nnegative_words = coeffs_df.sort_values('weight', ascending=True).head(20)\n#raise NotImplementedError()"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-9fa9de453a36f8d2","locked":false,"solution":true,"checksum":"b517fb83f670ea60b1fdb1580356a410","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"code","source":["positive_words.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div style=\"max-width:1500px;overflow:auto;\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weight</th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3555</th>\n      <td>0.590870</td>\n      <td>widelive.com/index.</td>\n    </tr>\n    <tr>\n      <th>12237</th>\n      <td>0.533567</td>\n      <td>08714712388</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.517100</td>\n      <td>call</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>0.513278</td>\n      <td>txt</td>\n    </tr>\n    <tr>\n      <th>9064</th>\n      <td>0.468274</td>\n      <td>gbp/sms</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["negative_words.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div style=\"max-width:1500px;overflow:auto;\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weight</th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>-0.162493</td>\n      <td>i</td>\n    </tr>\n    <tr>\n      <th>2444</th>\n      <td>-0.060939</td>\n      <td>fighting</td>\n    </tr>\n    <tr>\n      <th>3221</th>\n      <td>-0.059061</td>\n      <td>dificult</td>\n    </tr>\n    <tr>\n      <th>3371</th>\n      <td>-0.059061</td>\n      <td>fightng</td>\n    </tr>\n    <tr>\n      <th>3332</th>\n      <td>-0.059061</td>\n      <td>lose.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":43},{"cell_type":"markdown","source":["The `positive_words` and `negative_words` dataframe should look like this:\n\n```python\npositive_words.head()\n```\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3555</th>\n      <td>widelive.com/index.</td>\n      <td>0.590870</td>\n    </tr>\n    <tr>\n      <th>12237</th>\n      <td>08714712388</td>\n      <td>0.533567</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>call</td>\n      <td>0.517100</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>txt</td>\n      <td>0.513278</td>\n    </tr>\n    <tr>\n      <th>9064</th>\n      <td>gbp/sms</td>\n      <td>0.468274</td>\n    </tr>\n  </tbody>\n</table>\n\nand \n\n```python\nnegative_words.head()\n```\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>i</td>\n      <td>-0.162493</td>\n    </tr>\n    <tr>\n      <th>2444</th>\n      <td>fighting</td>\n      <td>-0.060939</td>\n    </tr>\n    <tr>\n      <th>3221</th>\n      <td>dificult</td>\n      <td>-0.059061</td>\n    </tr>\n    <tr>\n      <th>3371</th>\n      <td>fightng</td>\n      <td>-0.059061</td>\n    </tr>\n    <tr>\n      <th>3332</th>\n      <td>lose.</td>\n      <td>-0.059061</td>\n    </tr>\n  </tbody>\n</table>"],"metadata":{}},{"cell_type":"code","source":["# (15 pts)\nnp.testing.assert_equal(set(positive_words.columns), {'weight', 'word'})\nnp.testing.assert_equal(set(negative_words.columns), {'weight', 'word'})\nnp.testing.assert_approx_equal(positive_words.weight.sum(), 8.3701485692317927, significant=2)\nnp.testing.assert_approx_equal(negative_words.weight.sum(), -0.6661952507442954, significant=2)\nnp.testing.assert_array_less(positive_words.weight.iloc[-1], positive_words.weight.iloc[0])\nnp.testing.assert_array_less(negative_words.weight.iloc[0], negative_words.weight.iloc[-1])"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-5926b249c3d8d910","locked":true,"solution":false,"points":5,"checksum":"9d1c1cfaa5326d7403d0c48891dea0b0","grade":true}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":45},{"cell_type":"markdown","source":["# 1.7: Inference (15 pts)\nUse the dataframe `sms_spam3_df` to create a model where the first feature is `has_uppercase` and the next set of features are the tfidf of the text. Perform feature engineering in all features using a max absolute scaler ([`MaxAbsScaler`](https://spark.apache.org/docs/2.0.2/ml-features.html#maxabsscaler)). Do a logistic regression on the resulting scaled features with regularization parameter $\\lambda = 0.2$ and elastic net mixture $\\alpha=0.1$ for the entire data (all of `sms_spam3_df`). Since you have scaled all features to be within the same range, you can compare them. \n\n**(5 pts)** with code and comments, answer below\n\n1. is `has_uppercase` a feature that is positively or negative related to an SMS being spam?\n2. what is the ratio of the coefficient of `has_uppercase` to the biggest positive tfidf coefficient?"],"metadata":{}},{"cell_type":"code","source":["# your code and comments below\n# YOUR CODE HERE\nva = feature.VectorAssembler(inputCols = ['has_uppercase', 'tfidf'], outputCol = 'features')\nscale = feature.MaxAbsScaler(inputCol = 'features', outputCol = 'scaled_features')\nscaled_lr = LogisticRegression().setLabelCol('type').setFeaturesCol('scaled_features').setRegParam(0.2).setMaxIter(100).setElasticNetParam(0.1)\nscaled_pipeline = Pipeline(stages = [tokenizer, CounterVectorizer, IDF, va, scale, scaled_lr]).fit(sms_spam3_df)\n#raise NotImplementedError()"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-63ad341d276b5971","locked":false,"solution":true,"points":5,"checksum":"b811be2a38e793ee1cfdbc0c0624addb","grade":true}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":47},{"cell_type":"code","source":["scaled_pipeline.transform(sms_spam3_df).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\ntype|                text|has_uppercase|               words|                  tf|               tfidf|            features|     scaled_features|       rawPrediction|         probability|prediction|\n+----+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n   0|Go until jurong p...|            0|[go, until, juron...|(13525,[8,42,51,6...|(13525,[8,42,51,6...|(13526,[9,43,52,6...|(13526,[9,43,52,6...|[2.79089435542085...|[0.94218178424803...|       0.0|\n   0|Ok lar... Joking ...|            0|[ok, lar..., joki...|(13525,[5,74,404,...|(13525,[5,74,404,...|(13526,[6,75,405,...|(13526,[6,75,405,...|[2.79089435542085...|[0.94218178424803...|       0.0|\n   1|Free entry in 2 a...|            0|[free, entry, in,...|(13525,[0,3,8,20,...|(13525,[0,3,8,20,...|(13526,[1,4,9,21,...|(13526,[1,4,9,21,...|[-0.4357532631849...|[0.39275334554252...|       1.0|\n   0|U dun say so earl...|            0|[u, dun, say, so,...|(13525,[5,22,60,1...|(13525,[5,22,60,1...|(13526,[6,23,61,1...|(13526,[6,23,61,1...|[2.79089435542085...|[0.94218178424803...|       0.0|\n   0|Nah I don&apos;t think...|            0|[nah, i, don&apos;t, t...|(13525,[0,1,66,86...|(13525,[0,1,66,86...|(13526,[1,2,67,87...|(13526,[1,2,67,87...|[2.75817342641400...|[0.94037329790613...|       0.0|\n   1|FreeMsg Hey there...|            0|[freemsg, hey, th...|(13525,[0,2,6,10,...|(13525,[0,2,6,10,...|(13526,[1,3,7,11,...|(13526,[1,3,7,11,...|[2.12445996029986...|[0.89325792517319...|       0.0|\n   0|Even my brother i...|            0|[even, my, brothe...|(13525,[0,7,9,13,...|(13525,[0,7,9,13,...|(13526,[1,8,10,14...|(13526,[1,8,10,14...|[2.69317790057607...|[0.93662288524559...|       0.0|\n   0|As per your reque...|            0|[as, per, your, r...|(13525,[0,10,11,4...|(13525,[0,10,11,4...|(13526,[1,11,12,4...|(13526,[1,11,12,4...|[1.68978196261293...|[0.84419548368669...|       0.0|\n   1|WINNER!! As a val...|            1|[winner!!, as, a,...|(13525,[0,2,3,14,...|(13525,[0,2,3,14,...|(13526,[0,1,3,4,1...|(13526,[0,1,3,4,1...|[-1.7959385100822...|[0.14234618692169...|       1.0|\n   1|Had your mobile 1...|            1|[had, your, mobil...|(13525,[0,4,5,10,...|(13525,[0,4,5,10,...|(13526,[0,1,5,6,1...|(13526,[0,1,5,6,1...|[-1.1233525119169...|[0.24538995665625...|       1.0|\n   0|I&apos;m gonna be home...|            0|[i&apos;m, gonna, be, ...|(13525,[0,1,6,29,...|(13525,[0,1,6,29,...|(13526,[1,2,7,30,...|(13526,[1,2,7,30,...|[2.75817342641400...|[0.94037329790613...|       0.0|\n   1|SIX chances to wi...|            1|[six, chances, to...|(13525,[0,6,40,48...|(13525,[0,6,40,48...|(13526,[0,1,7,41,...|(13526,[0,1,7,41,...|[-0.3862113927038...|[0.40462966249821...|       1.0|\n   1|URGENT! You have ...|            1|[urgent!, you, ha...|(13525,[0,2,3,4,8...|(13525,[0,2,3,4,8...|(13526,[0,1,3,4,5...|(13526,[0,1,3,4,5...|[-1.5702246897558...|[0.17218436279181...|       1.0|\n   0|I&apos;ve been searchi...|            0|[i&apos;ve, been, sear...|(13525,[0,1,2,3,4...|(13525,[0,1,2,3,4...|(13526,[1,2,3,4,5...|(13526,[1,2,3,4,5...|[2.47532707061892...|[0.92239395348824...|       0.0|\n   0|I HAVE A DATE ON ...|            1|[i, have, a, date...|(13525,[1,3,14,16...|(13525,[1,3,14,16...|(13526,[0,2,4,15,...|(13526,[0,2,4,15,...|[1.90544195305308...|[0.87050620846253...|       0.0|\n   1|XXXMobileMovieClu...|            1|[xxxmobilemoviecl...|(13525,[0,4,8,11,...|(13525,[0,4,8,11,...|(13526,[0,1,5,9,1...|(13526,[0,1,5,9,1...|[0.50769578645279...|[0.62426615787796...|       0.0|\n   0|Oh k...i&apos;m watchi...|            0|[oh, k...i&apos;m, wat...|(13525,[159,314,4...|(13525,[159,314,4...|(13526,[160,315,4...|(13526,[160,315,4...|[2.79089435542085...|[0.94218178424803...|       0.0|\n   0|Eh u remember how...|            0|[eh, u, remember,...|(13525,[1,5,20,46...|(13525,[1,5,20,46...|(13526,[2,6,21,47...|(13526,[2,6,21,47...|[2.85847497069922...|[0.94575511505416...|       0.0|\n   0|Fine if that&apos;s th...|            0|[fine, if, that&apos;s...|(13525,[4,5,30,59...|(13525,[4,5,30,59...|(13526,[5,6,31,60...|(13526,[5,6,31,60...|[2.79089435542085...|[0.94218178424803...|       0.0|\n   1|England v Macedon...|            1|[england, v, mace...|(13525,[0,4,28,81...|(13525,[0,4,28,81...|(13526,[0,1,5,29,...|(13526,[0,1,5,29,...|[0.44188036277409...|[0.60870699223314...|       0.0|\n+----+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":48},{"cell_type":"code","source":["has_uppercase_coeff= scaled_pipeline.stages[-1].coefficients\nhas_uppercase_coeff\n# 1. Since the coefficient value of 'has_uppercase' is 0.92 which is a positive value, it is a feature that is positively related to an SMS being spam."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">130</span><span class=\"ansired\">]: </span>SparseVector(13526, {0: 0.9289, 1: 0.9174, 2: -0.7376, 4: 0.154, 10: -0.0625, 11: 0.2256, 12: 0.875, 14: -0.047, 16: 2.0119, 21: 0.3854, 25: 0.6186, 29: 0.3159, 41: 0.5569, 49: 0.1865, 50: 0.1992, 56: 1.2697, 64: 0.0684, 69: 1.4531, 82: 1.5802, 86: 0.4017, 88: 0.5251, 96: 1.0955, 97: 1.4391, 102: 1.0041, 103: 0.2397, 109: 1.3385, 110: 1.3782, 111: 0.0415, 131: 0.65, 154: 0.7224, 168: 1.3173, 170: 0.2842, 171: 0.4273, 179: 0.8028, 180: 0.6217, 181: 0.1321, 204: 0.6288, 213: 0.5852, 224: 0.6594, 234: 0.8617, 235: 0.2618, 262: 0.0258, 263: 0.1595, 274: 0.2208, 276: 0.6916, 286: 0.597, 289: 0.6877, 291: 0.4626, 302: 0.2451, 308: 0.0278, 318: 0.1918, 331: 0.2985, 334: 0.0142, 345: 0.0801, 353: 0.0593, 360: 0.3731, 383: 0.1629, 384: 0.0089, 399: 0.4152, 403: 0.2221, 428: 0.3498, 431: 0.0381, 443: 0.2669, 449: 0.7368, 454: 0.1396, 456: 0.5005, 458: 0.0099, 465: 0.5622, 483: 0.1408, 485: 0.1848, 486: 0.1238, 492: 0.1117, 497: 0.3971, 507: 0.1277, 527: 0.3345, 535: 0.1307, 542: 0.6873, 547: 0.0276, 549: 0.1408, 553: 0.4842, 578: 0.2195, 606: 0.2999, 608: 0.1159, 623: 0.115, 627: 0.0476, 628: 0.2006, 641: 0.0433, 645: 0.4791, 647: 0.2473, 658: 0.2473, 666: 0.0156, 670: 0.1412, 675: 0.1412, 685: 0.0634, 698: 0.0994, 703: 0.2858, 718: 0.6274, 721: 0.0087, 725: 0.2384, 728: 0.3661, 759: 0.0108, 766: 0.0412, 784: 0.2112, 788: 0.1715, 791: 0.0938, 793: 0.2477, 831: 0.2063, 834: 0.1537, 897: 0.03, 898: 0.0159, 907: 0.0209, 918: 0.0307, 951: 0.0075, 1080: 0.2071, 1161: 0.4536, 1168: 0.4536, 1170: 0.0093, 1182: 0.1814, 1196: 0.4536, 1197: 0.0981, 1210: 0.1772, 1278: 0.4536, 1284: 0.0387, 1352: 0.3654, 1396: 0.0927, 1462: 0.0899, 1483: 0.0491, 1550: 0.0945, 1619: 0.3918, 1624: 0.0945, 1756: 0.0325, 1816: 0.1579, 2134: 0.0747, 2300: 0.0423})</div>"]}}],"execution_count":49},{"cell_type":"code","source":["#Biggest positive tfidf coefficient\nmax_coeff = max(scaled_pipeline.stages[-1].coefficients)\nratio=(has_uppercase_coeff/max_coeff)\nratio\n# 2. The ratio of the coefficient of has_uppercase to the biggest positive tfidf coefficient is 0.46."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">124</span><span class=\"ansired\">]: </span>0.46170057983376228</div>"]}}],"execution_count":50},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":51}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.4","nbconvert_exporter":"python","file_extension":".py"},"name":"spam_detection","notebookId":781376812484838,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"celltoolbar":"Edit Metadata"},"nbformat":4,"nbformat_minor":0}
